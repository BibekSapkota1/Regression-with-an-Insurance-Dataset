{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":84896,"databundleVersionId":10305135,"sourceType":"competition"},{"sourceId":10334226,"sourceType":"datasetVersion","datasetId":6398872},{"sourceId":10334327,"sourceType":"datasetVersion","datasetId":6398946},{"sourceId":10334337,"sourceType":"datasetVersion","datasetId":6398955},{"sourceId":10334339,"sourceType":"datasetVersion","datasetId":6398957},{"sourceId":10334344,"sourceType":"datasetVersion","datasetId":6398960}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nimport xgboost as xgb\nimport numpy as np\nimport pandas as pd\nfrom statistics import mean\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport numpy as np\nimport pandas as pd\nimport xgboost as xgb\nimport optuna\nimport xgboost as xgb\nimport numpy as np\nimport pandas as pd\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import mean_squared_log_error","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:18:00.803855Z","iopub.execute_input":"2024-12-30T12:18:00.804311Z","iopub.status.idle":"2024-12-30T12:18:00.813644Z","shell.execute_reply.started":"2024-12-30T12:18:00.804273Z","shell.execute_reply":"2024-12-30T12:18:00.812189Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/playground-series-s4e12/test.csv')\ntest_df = pd.read_csv('/kaggle/input/playground-series-s4e12/train.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:18:00.815659Z","iopub.execute_input":"2024-12-30T12:18:00.816150Z","iopub.status.idle":"2024-12-30T12:18:09.625636Z","shell.execute_reply.started":"2024-12-30T12:18:00.816109Z","shell.execute_reply":"2024-12-30T12:18:09.624764Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"def date_trans(df):\n    df['Policy Start Date']= pd.to_datetime(df['Policy Start Date'])\n    df['Year'] = df['Policy Start Date'].dt.year\n    df['Day'] = df['Policy Start Date'].dt.day\n    df['Month'] = df['Policy Start Date'].dt.month\n    df['DayOfWeek'] = df['Policy Start Date'].dt.dayofweek\n    df.drop('Policy Start Date' , axis =1, inplace = True)\n    return df\n\ntrain_df = date_trans(train_df)\ntest_df = date_trans(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:18:09.627801Z","iopub.execute_input":"2024-12-30T12:18:09.628171Z","iopub.status.idle":"2024-12-30T12:18:11.142981Z","shell.execute_reply.started":"2024-12-30T12:18:09.628144Z","shell.execute_reply":"2024-12-30T12:18:11.141880Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def rmsle(y_true, y_pred):\n    return np.sqrt(mean_squared_log_error(y_true, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:18:11.144344Z","iopub.execute_input":"2024-12-30T12:18:11.144627Z","iopub.status.idle":"2024-12-30T12:18:11.149556Z","shell.execute_reply.started":"2024-12-30T12:18:11.144603Z","shell.execute_reply":"2024-12-30T12:18:11.148278Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Assume train and test are already loaded as DataFrames\nfor col in train_df.select_dtypes(include='object').columns:\n    train_df[col] = train_df[col].astype('category')\nfor col in test_df.select_dtypes(include='object').columns:\n    test_df[col] = test_df[col].astype('category')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:18:11.150604Z","iopub.execute_input":"2024-12-30T12:18:11.151003Z","iopub.status.idle":"2024-12-30T12:18:13.421628Z","shell.execute_reply.started":"2024-12-30T12:18:11.150964Z","shell.execute_reply":"2024-12-30T12:18:13.420387Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# Xg Boost","metadata":{}},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.preprocessing import KBinsDiscretizer\n# from sklearn.metrics import mean_squared_error, mean_squared_log_error\n# from xgboost import XGBRegressor\n# from numpy import log1p, expm1\n\n# # Set random seed\n# np.random.seed(42)\n\n# # Assume train_df and test_df are your DataFrames\n# df = train_df\n\n# # Features and target\n# X = df.drop('Premium Amount', axis=1)\n# y = df['Premium Amount']\n\n# # Log transform the target\n# y_log = np.log1p(y)\n\n# # Stratify target by binning into discrete intervals\n# binner = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n# y_binned_log = binner.fit_transform(y_log.values.reshape(-1, 1)).astype(int).ravel()\n\n# # Stratified K-Fold\n# n_splits = 5\n# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# # Arrays to store predictions\n# oof_predictions = np.zeros(len(X))  # Out-of-fold predictions\n# test_predictions = np.zeros(len(X))  # Test predictions for each fold\n\n# # Test set\n# X_test = test_df\n# test_preds_per_fold = np.zeros((len(X_test), n_splits))  # Store test predictions per fold\n# fold_oof_results = []  # To store ID, target, and OOF predictions\n\n# # XGBoost Regressor parameters\n# model_params = {'n_estimators': 7000,\n#                 'learning_rate': 0.0037894924041441663,\n#                 'max_depth': 9,\n#                 'min_child_weight': 7,\n#                 'gamma': 0.003166869962093635,\n#                 'subsample': 0.8491559357878403,\n#                 'colsample_bytree': 0.9931791435553496,\n#                 'reg_alpha': 8.030670352805062e-08,\n#                 'reg_lambda': 0.23939002451629704,\n#                 \"random_state\": 42,\n#                 'eval_metric': 'rmse',\n#                 \"objective\": \"reg:squarederror\",  # XGBoost objective for regression\n#                 \"tree_method\": \"hist\",  # Use GPU acceleration\n#                 \"device\": \"cuda\",\n#                 \"enable_categorical\": True  # Enable categorical handling if required\n#                }\n\n# # Cross-validation loop\n# for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y_binned_log)):\n#     print(f\"Fold {fold + 1}/{n_splits}\")\n    \n#     # Split data\n#     X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n#     y_train, y_valid = y_log.iloc[train_idx], y_log.iloc[valid_idx]\n    \n#     # Model\n#     model = XGBRegressor(**model_params, early_stopping_rounds=300)\n#     model.fit(X_train, y_train,\n#               eval_set=[(X_valid, y_valid)],\n#               verbose=0)\n    \n#     # Predictions (log transformed)\n#     log_oof_preds = model.predict(X_valid)\n    \n#     # Revert log transformation\n#     oof_predictions[valid_idx] = np.expm1(log_oof_preds)\n\n#     fold_rmsle = rmsle(y.iloc[valid_idx], oof_predictions[valid_idx])\n#     print(f\"Fold {fold + 1} RMSLE: {fold_rmsle:.4f}\")\n    \n#     # Store fold results\n#     fold_result = pd.DataFrame({\n#         'ID': X.index[valid_idx],\n#         'Actual': y.iloc[valid_idx],\n#         'OOF_Pred_XGB': oof_predictions[valid_idx],\n#         'Fold': fold + 1\n#     })\n#     fold_oof_results.append(fold_result)\n    \n#     # Test set predictions\n#     log_test_preds = model.predict(X_test)\n#     test_preds_per_fold[:, fold] = np.expm1(log_test_preds)\n\n# # Combine fold results\n# oof_results_df = pd.concat(fold_oof_results, axis=0, ignore_index=True)\n\n# # Average predictions on test data\n# final_test_predictions = test_preds_per_fold.mean(axis=1)\n\n# # Evaluate OOF predictions\n# oof_mse = mean_squared_error(y, oof_predictions)\n# oof_rmsle = np.sqrt(mean_squared_log_error(y, oof_predictions))\n\n# print(f\"OOF Mean Squared Error: {oof_mse:.4f}\")\n# print(f\"OOF Root Mean Squared Log Error: {oof_rmsle:.4f}\")\n\n# # Output predictions\n# print(\"Final Test Predictions:\", final_test_predictions)\n# print(oof_results_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:18:13.422908Z","iopub.execute_input":"2024-12-30T12:18:13.423347Z","iopub.status.idle":"2024-12-30T12:18:13.429085Z","shell.execute_reply.started":"2024-12-30T12:18:13.423306Z","shell.execute_reply":"2024-12-30T12:18:13.428018Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# sub = pd.read_csv('playground-series-s4e12/sample_submission.csv')\n# sub['Premium Amount'] = final_test_predictions\n# sub.to_csv('submission.csv', index=False)\n# oof_results_df.to_csv('oof_xgb.csv',index = False)\n# sub.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:18:13.430213Z","iopub.execute_input":"2024-12-30T12:18:13.430482Z","iopub.status.idle":"2024-12-30T12:18:13.455828Z","shell.execute_reply.started":"2024-12-30T12:18:13.430459Z","shell.execute_reply":"2024-12-30T12:18:13.454775Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# Light Boost","metadata":{}},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.preprocessing import KBinsDiscretizer\n# from lightgbm import LGBMRegressor\n# from sklearn.metrics import mean_squared_error, mean_squared_log_error\n\n# def log_transform(y):\n#     \"\"\"\n#     Apply log transformation safely, handling zero and negative values\n#     \"\"\"\n#     return np.log1p(y)\n\n# def inverse_log_transform(y_log):\n#     \"\"\"\n#     Revert log transformation\n#     \"\"\"\n#     return np.expm1(y_log)\n\n# # Example dataset (replace with your dataset)\n# np.random.seed(42)\n# df = train_df\n\n# # Find the target column (assumes it contains 'Premium' or 'Amount')\n# target_column = [col for col in df.columns if 'premium' in col.lower() or 'amount' in col.lower()]\n# if not target_column:\n#     raise ValueError(\"Could not find target column. Please specify the column name for premium/amount.\")\n# target_column = target_column[0]\n\n# # Identify column types\n# numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n# categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n\n# # Remove target variable from features\n# if target_column in numeric_features:\n#     numeric_features.remove(target_column)\n# if target_column in categorical_features:\n#     categorical_features.remove(target_column)\n\n# # Features and target\n# X = df.drop(columns=[target_column])\n# y = df[target_column]\n\n# # Log transform the target variable\n# y_log = log_transform(y)\n\n# # Stratify target by binning into discrete intervals\n# binner = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n# y_binned = binner.fit_transform(y_log.values.reshape(-1, 1)).astype(int).ravel()\n\n# # Stratified K-Fold\n# n_splits = 5\n# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# # Arrays to store predictions\n# oof_predictions_log = np.zeros(len(X))  # Out-of-fold predictions in log space\n# oof_predictions = np.zeros(len(X))  # Out-of-fold predictions in original space\n\n# # Test set (replace with your actual test set)\n# X_test = test_df\n\n# # Prepare for cross-validation\n# test_preds_per_fold_log = np.zeros((len(X_test), n_splits))  # Store test predictions per fold in log space\n# test_preds_per_fold = np.zeros((len(X_test), n_splits))  # Store test predictions per fold in original space\n# fold_oof_results = []  # To store ID, target, and OOF predictions\n\n# # LightGBM Regressor parameters\n# model_params = {\n#     'objective': 'regression',\n#     'metric': 'rmsle',\n#     'random_state': 42,\n#     'is_unbalance': True,\n#       'nan_as_missing': True,\n#       'n_estimators': 2900, \n#       'learning_rate': 0.009292502487271825,\n#       'max_depth': 11,\n#       'num_leaves': 88, \n#       'min_child_samples': 46,\n#       'min_child_weight': 0.0008888702484611456, \n#       'subsample': 0.8552895795397466, \n#       'subsample_freq': 5, \n#       'colsample_bytree': 0.9135439280459978,\n#       'reg_alpha': 0.0021100979917979567,\n#       'reg_lambda': 5.0153235478869446e-08,\n#       'verbose': -1,\n#         'device': 'gpu',\n#         'gpu_platform_id': 0,\n#         'gpu_device_id': 0,\n#         'gpu_use_dp': True  \n# }\n\n# # Cross-validation loop\n# for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y_binned)):\n#     print(f\"Fold {fold + 1}/{n_splits}\")\n#     X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n#     y_train_log, y_valid_log = y_log.iloc[train_idx], y_log.iloc[valid_idx]\n#     y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n    \n#     # Model with early stopping on log-transformed target\n#     model = LGBMRegressor(**{**model_params, 'verbose': -1})\n#     model.fit(X_train, y_train_log, \n#               eval_set=[(X_valid, y_valid_log)],\n#               )\n    \n#     # Predictions in log space\n#     oof_predictions_log[valid_idx] = model.predict(X_valid)\n    \n#     # Convert log predictions back to original space\n#     oof_predictions[valid_idx] = inverse_log_transform(oof_predictions_log[valid_idx])\n\n#     fold_rmsle = rmsle(y.iloc[valid_idx], oof_predictions[valid_idx])\n#     print(f\"Fold {fold + 1} RMSLE: {fold_rmsle:.4f}\")\n    \n#     # Store fold results with ID, target, and OOF predictions\n#     fold_result = pd.DataFrame({\n#         'ID': X.index[valid_idx],       # Assuming X has an index as IDs\n#         'Actual': y.iloc[valid_idx],    # Actual target values\n#         'OOF_Pred_LGB': oof_predictions[valid_idx],    # OOF predictions\n#         'Fold': fold + 1               # Fold number\n#     })\n#     fold_oof_results.append(fold_result)\n    \n#     # Test set predictions for this fold\n#     test_preds_per_fold_log[:, fold] = model.predict(X_test)\n#     test_preds_per_fold[:, fold] = inverse_log_transform(test_preds_per_fold_log[:, fold])\n\n# # Combine OOF results\n# oof_results_df = pd.concat(fold_oof_results, axis=0, ignore_index=True)\n\n# # Average predictions on test data\n# final_test_predictions = test_preds_per_fold.mean(axis=1)\n\n# # Evaluate OOF predictions\n# oof_mse = mean_squared_error(y, oof_predictions)\n# oof_rmsle = np.sqrt(mean_squared_log_error(y, oof_predictions))\n\n# print(f\"OOF Mean Squared Error: {oof_mse:.4f}\")\n# print(f\"OOF Root Mean Squared Log Error: {oof_rmsle:.4f}\")\n\n# print(\"Final Test Predictions:\", final_test_predictions)\n# print(oof_results_df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:18:13.458408Z","iopub.execute_input":"2024-12-30T12:18:13.458698Z","iopub.status.idle":"2024-12-30T12:18:13.483346Z","shell.execute_reply.started":"2024-12-30T12:18:13.458674Z","shell.execute_reply":"2024-12-30T12:18:13.482193Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# sub = pd.read_csv('playground-series-s4e12/sample_submission.csv')\n# sub['Premium Amount'] = final_test_predictions\n# sub.to_csv('submission.csv', index=False)\n# sub.head()\n# oof_results_df.to_csv('oof_lgbm.csv',index = False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:18:13.484868Z","iopub.execute_input":"2024-12-30T12:18:13.485282Z","iopub.status.idle":"2024-12-30T12:18:13.509142Z","shell.execute_reply.started":"2024-12-30T12:18:13.485243Z","shell.execute_reply":"2024-12-30T12:18:13.507927Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"# Cat Boost","metadata":{}},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.preprocessing import KBinsDiscretizer\n# from catboost import CatBoostRegressor\n# from sklearn.metrics import mean_squared_error, mean_squared_log_error\n\n# def log_transform(y):\n#     \"\"\"\n#     Apply log transformation safely, handling zero and negative values\n#     \"\"\"\n#     return np.log1p(y)\n\n# def inverse_log_transform(y_log):\n#     \"\"\"\n#     Revert log transformation\n#     \"\"\"\n#     return np.expm1(y_log)\n\n# # Example dataset (replace with your dataset)\n# np.random.seed(42)\n# df = train_df\n\n# # Find the target column (assumes it contains 'Premium' or 'Amount')\n# target_column = [col for col in df.columns if 'premium' in col.lower() or 'amount' in col.lower()]\n# if not target_column:\n#     raise ValueError(\"Could not find target column. Please specify the column name for premium/amount.\")\n# target_column = target_column[0]\n\n# # Identify column types\n# numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n# categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n\n# # Remove target variable from features\n# if target_column in numeric_features:\n#     numeric_features.remove(target_column)\n# if target_column in categorical_features:\n#     categorical_features.remove(target_column)\n\n# # Features and target\n# X = df.drop(columns=[target_column])\n# y = df[target_column]\n\n# # Log transform the target variable\n# y_log = log_transform(y)\n\n# # Stratify target by binning into discrete intervals\n# binner = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n# y_binned = binner.fit_transform(y_log.values.reshape(-1, 1)).astype(int).ravel()\n\n# # Stratified K-Fold\n# n_splits = 5\n# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# # Arrays to store predictions\n# oof_predictions_log = np.zeros(len(X))  # Out-of-fold predictions in log space\n# oof_predictions = np.zeros(len(X))  # Out-of-fold predictions in original space\n\n# # Test set (replace with your actual test set)\n# X_test = test_df\n\n# # Prepare for cross-validation\n# test_preds_per_fold_log = np.zeros((len(X_test), n_splits))  # Store test predictions per fold in log space\n# test_preds_per_fold = np.zeros((len(X_test), n_splits))  # Store test predictions per fold in original space\n# fold_oof_results = []  # To store ID, target, and OOF predictions\n\n# # CatBoost Regressor parameters\n# model_params = {\n#     'loss_function': 'RMSE',\n#     'eval_metric': 'RMSLE',\n#     'random_seed': 42,\n#     'iterations': 2900,\n#     'learning_rate': 0.009292502487271825,\n#     'depth': 11,\n#     'l2_leaf_reg': 3.5,\n#     'subsample': 0.8552895795397466,\n#     'colsample_bylevel': 0.9135439280459978,\n#     'bootstrap_type': 'Bernoulli',\n#     'verbose': False,\n#     'task_type': 'GPU',\n#     'devices': '0'\n# }\n\n# # Cross-validation loop\n# for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y_binned)):\n#     print(f\"Fold {fold + 1}/{n_splits}\")\n#     X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n#     y_train_log, y_valid_log = y_log.iloc[train_idx], y_log.iloc[valid_idx]\n#     y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n    \n#     # Model with early stopping on log-transformed target\n#     model = CatBoostRegressor(**model_params)\n#     model.fit(X_train, y_train_log, \n#               eval_set=(X_valid, y_valid_log),\n#               use_best_model=True)\n    \n#     # Predictions in log space\n#     oof_predictions_log[valid_idx] = model.predict(X_valid)\n    \n#     # Convert log predictions back to original space\n#     oof_predictions[valid_idx] = inverse_log_transform(oof_predictions_log[valid_idx])\n\n#     fold_rmsle = np.sqrt(mean_squared_log_error(y.iloc[valid_idx], oof_predictions[valid_idx]))\n#     print(f\"Fold {fold + 1} RMSLE: {fold_rmsle:.4f}\")\n    \n#     # Store fold results with ID, target, and OOF predictions\n#     fold_result = pd.DataFrame({\n#         'ID': X.index[valid_idx],       # Assuming X has an index as IDs\n#         'Actual': y.iloc[valid_idx],    # Actual target values\n#         'OOF_Pred_CatBoost': oof_predictions[valid_idx],    # OOF predictions\n#         'Fold': fold + 1               # Fold number\n#     })\n#     fold_oof_results.append(fold_result)\n    \n#     # Test set predictions for this fold\n#     test_preds_per_fold_log[:, fold] = model.predict(X_test)\n#     test_preds_per_fold[:, fold] = inverse_log_transform(test_preds_per_fold_log[:, fold])\n\n# # Combine OOF results\n# oof_results_df = pd.concat(fold_oof_results, axis=0, ignore_index=True)\n\n# # Average predictions on test data\n# final_test_predictions = test_preds_per_fold.mean(axis=1)\n\n# # Evaluate OOF predictions\n# oof_mse = mean_squared_error(y, oof_predictions)\n# oof_rmsle = np.sqrt(mean_squared_log_error(y, oof_predictions))\n\n# print(f\"OOF Mean Squared Error: {oof_mse:.4f}\")\n# print(f\"OOF Root Mean Squared Log Error: {oof_rmsle:.4f}\")\n\n# print(\"Final Test Predictions:\", final_test_predictions)\n# print(oof_results_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:18:13.510326Z","iopub.execute_input":"2024-12-30T12:18:13.510742Z","iopub.status.idle":"2024-12-30T12:18:13.527646Z","shell.execute_reply.started":"2024-12-30T12:18:13.510703Z","shell.execute_reply":"2024-12-30T12:18:13.526520Z"}},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":"# Linear Regression","metadata":{}},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.preprocessing import KBinsDiscretizer, OneHotEncoder\n# from sklearn.linear_model import LinearRegression\n# from sklearn.metrics import mean_squared_error, mean_squared_log_error\n# from sklearn.compose import ColumnTransformer\n# from sklearn.pipeline import Pipeline\n# from sklearn.impute import SimpleImputer\n\n# def log_transform(y):\n#     \"\"\"\n#     Apply log transformation safely, handling zero and negative values\n#     \"\"\"\n#     return np.log1p(y)\n\n# def inverse_log_transform(y_log):\n#     \"\"\"\n#     Revert log transformation\n#     \"\"\"\n#     return np.expm1(y_log)\n\n# def rmsle(y_true, y_pred):\n#     \"\"\"\n#     Calculate Root Mean Squared Logarithmic Error\n#     \"\"\"\n#     return np.sqrt(mean_squared_log_error(y_true, y_pred))\n\n# # Example dataset (replace with your dataset)\n# np.random.seed(42)\n# df = train_df\n\n# # Find the target column (assumes it contains 'Premium' or 'Amount')\n# target_column = [col for col in df.columns if 'premium' in col.lower() or 'amount' in col.lower()]\n# if not target_column:\n#     raise ValueError(\"Could not find target column. Please specify the column name for premium/amount.\")\n# target_column = target_column[0]\n\n# # Identify column types\n# numeric_features = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n# categorical_features = df.select_dtypes(include=['object', 'category']).columns.tolist()\n\n# # Remove target variable from features\n# if target_column in numeric_features:\n#     numeric_features.remove(target_column)\n# if target_column in categorical_features:\n#     categorical_features.remove(target_column)\n\n# # Features and target\n# X = df.drop(columns=[target_column])\n# y = df[target_column]\n\n# # Log transform the target variable\n# y_log = log_transform(y)\n\n# # Stratify target by binning into discrete intervals\n# binner = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n# y_binned = binner.fit_transform(y_log.values.reshape(-1, 1)).astype(int).ravel()\n\n# # Test set (replace with your actual test set)\n# X_test = test_df\n\n# # Preprocessing pipeline\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         ('num', SimpleImputer(strategy='median'), numeric_features),\n#         ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n#     ]\n# )\n\n# # LinearRegression model parameters (no hyperparameters to tune)\n# model = LinearRegression()\n\n# # Stratified K-Fold\n# n_splits = 5\n# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# # Arrays to store predictions\n# oof_predictions_log = np.zeros(len(X))  # Out-of-fold predictions in log space\n# oof_predictions = np.zeros(len(X))  # Out-of-fold predictions in original space\n\n# # Prepare for cross-validation\n# test_preds_per_fold_log = np.zeros((len(X_test), n_splits))  # Store test predictions per fold in log space\n# test_preds_per_fold = np.zeros((len(X_test), n_splits))  # Store test predictions per fold in original space\n# fold_oof_results = []  # To store ID, target, and OOF predictions\n\n# # Cross-validation loop\n# for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y_binned)):\n#     print(f\"Fold {fold + 1}/{n_splits}\")\n#     X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n#     y_train_log, y_valid_log = y_log.iloc[train_idx], y_log.iloc[valid_idx]\n#     y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n    \n#     # Model pipeline with preprocessing and regressor\n#     pipeline = Pipeline(steps=[\n#         ('preprocessor', preprocessor),\n#         ('model', model)\n#     ])\n    \n#     # Fit the model\n#     pipeline.fit(X_train, y_train_log)\n    \n#     # Predictions in log space\n#     oof_predictions_log[valid_idx] = pipeline.predict(X_valid)\n    \n#     # Convert log predictions back to original space\n#     oof_predictions[valid_idx] = inverse_log_transform(oof_predictions_log[valid_idx])\n\n#     fold_rmsle = rmsle(y.iloc[valid_idx], oof_predictions[valid_idx])\n#     print(f\"Fold {fold + 1} RMSLE: {fold_rmsle:.4f}\")\n    \n#     # Store fold results with ID, target, and OOF predictions\n#     fold_result = pd.DataFrame({\n#         'ID': X.index[valid_idx],       # Assuming X has an index as IDs\n#         'Actual': y.iloc[valid_idx],    # Actual target values\n#         'OOF_Pred_LR': oof_predictions[valid_idx],    # OOF predictions\n#         'Fold': fold + 1               # Fold number\n#     })\n#     fold_oof_results.append(fold_result)\n    \n#     # Test set predictions for this fold\n#     test_preds_per_fold_log[:, fold] = pipeline.predict(X_test)\n#     test_preds_per_fold[:, fold] = inverse_log_transform(test_preds_per_fold_log[:, fold])\n\n# # Combine OOF results\n# oof_results_df = pd.concat(fold_oof_results, axis=0, ignore_index=True)\n\n# # Average predictions on test data\n# final_test_predictions = test_preds_per_fold.mean(axis=1)\n\n# # Evaluate OOF predictions\n# oof_mse = mean_squared_error(y, oof_predictions)\n# oof_rmsle = np.sqrt(mean_squared_log_error(y, oof_predictions))\n\n# print(f\"OOF Mean Squared Error: {oof_mse:.4f}\")\n# print(f\"OOF Root Mean Squared Log Error: {oof_rmsle:.4f}\")\n\n# print(\"Final Test Predictions:\", final_test_predictions)\n# print(oof_results_df.head())\n\n\n# sub = pd.read_csv('/kaggle/input/playground-series-s4e12/sample_submission.csv')\n# sub['Premium Amount'] = final_test_predictions\n# sub.to_csv('lrsubmission.csv', index=False)\n# sub.head()\n# oof_results_df.to_csv('oof_lr.csv',index = False)\n# sub.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:18:13.528724Z","iopub.execute_input":"2024-12-30T12:18:13.529087Z","iopub.status.idle":"2024-12-30T12:18:13.550438Z","shell.execute_reply.started":"2024-12-30T12:18:13.529058Z","shell.execute_reply":"2024-12-30T12:18:13.549481Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"# sub = pd.read_csv('/kaggle/input/playground-series-s4e12/sample_submission.csv')\n# sub['Premium Amount'] = final_test_predictions\n# sub.to_csv('lrsubmission.csv', index=False)\n# sub.head()\n# oof_results_df.to_csv('oof_lr.csv',index = False)\n# sub.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:18:13.551545Z","iopub.execute_input":"2024-12-30T12:18:13.551953Z","iopub.status.idle":"2024-12-30T12:18:13.572686Z","shell.execute_reply.started":"2024-12-30T12:18:13.551893Z","shell.execute_reply":"2024-12-30T12:18:13.571616Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"# Aada Boost","metadata":{}},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# from sklearn.model_selection import StratifiedKFold\n# from sklearn.preprocessing import KBinsDiscretizer, OneHotEncoder\n# from sklearn.metrics import mean_squared_error, mean_squared_log_error\n# from sklearn.ensemble import AdaBoostRegressor\n# from sklearn.tree import DecisionTreeRegressor\n# from sklearn.impute import SimpleImputer\n# from sklearn.compose import ColumnTransformer\n# from sklearn.pipeline import Pipeline\n\n# # Set random seed\n# np.random.seed(42)\n\n# # Assume train_df and test_df are your DataFrames\n# df = train_df\n\n# # Features and target\n# X = df.drop('Premium Amount', axis=1)\n# y = df['Premium Amount']\n\n# # Log transform the target\n# y_log = np.log1p(y)\n\n# # Stratify target by binning into discrete intervals\n# binner = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\n# y_binned_log = binner.fit_transform(y_log.values.reshape(-1, 1)).astype(int).ravel()\n\n# # Identify categorical and numerical columns\n# numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n# categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n\n# # Create imputers for numerical and categorical columns\n# numerical_imputer = SimpleImputer(strategy='mean')  # Use mean for numerical features\n# categorical_imputer = SimpleImputer(strategy='most_frequent')  # Use most frequent for categorical features\n# categorical_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)  # One hot encoding for categorical features\n\n# # Preprocessing pipeline\n# preprocessor = ColumnTransformer(\n#     transformers=[\n#         ('num', numerical_imputer, numerical_cols),\n#         ('cat', Pipeline([\n#             ('imputer', categorical_imputer),\n#             ('encoder', categorical_encoder)\n#         ]), categorical_cols)\n#     ]\n# )\n\n# # Stratified K-Fold\n# n_splits = 5\n# skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n# # Arrays to store predictions\n# oof_predictions = np.zeros(len(X))  # Out-of-fold predictions\n# test_predictions = np.zeros(len(X))  # Test predictions for each fold\n\n# # Test set\n# X_test = test_df\n# test_preds_per_fold = np.zeros((len(X_test), n_splits))  # Store test predictions per fold\n# fold_oof_results = []  # To store ID, target, and OOF predictions\n\n# # Cross-validation loop\n# for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y_binned_log)):\n#     print(f\"Fold {fold + 1}/{n_splits}\")\n    \n#     # Split data\n#     X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n#     y_train, y_valid = y_log.iloc[train_idx], y_log.iloc[valid_idx]\n    \n#     # Apply preprocessing to both train and valid sets\n#     X_train_transformed = preprocessor.fit_transform(X_train)\n#     X_valid_transformed = preprocessor.transform(X_valid)\n    \n#     # Model\n#     model = AdaBoostRegressor(\n#         base_estimator=DecisionTreeRegressor(max_depth=5),\n#         n_estimators=100,\n#         learning_rate=0.1,\n#         random_state=42\n#     )\n#     model.fit(X_train_transformed, y_train)\n    \n#     # Predictions (log transformed)\n#     log_oof_preds = model.predict(X_valid_transformed)\n    \n#     # Revert log transformation\n#     oof_predictions[valid_idx] = np.expm1(log_oof_preds)\n\n#     fold_rmsle = np.sqrt(mean_squared_log_error(y.iloc[valid_idx], oof_predictions[valid_idx]))\n#     print(f\"Fold {fold + 1} RMSLE: {fold_rmsle:.4f}\")\n    \n#     # Store fold results\n#     fold_result = pd.DataFrame({\n#         'ID': X.index[valid_idx],\n#         'Actual': y.iloc[valid_idx],\n#         'OOF_Pred': oof_predictions[valid_idx],\n#         'Fold': fold + 1\n#     })\n#     fold_oof_results.append(fold_result)\n    \n#     # Test set predictions (transform test data)\n#     X_test_transformed = preprocessor.transform(X_test)\n#     log_test_preds = model.predict(X_test_transformed)\n#     test_preds_per_fold[:, fold] = np.expm1(log_test_preds)\n\n# # Combine fold results\n# oof_results_df = pd.concat(fold_oof_results, axis=0, ignore_index=True)\n\n# # Average predictions on test data\n# final_test_predictions = test_preds_per_fold.mean(axis=1)\n\n# # Evaluate OOF predictions\n# oof_mse = mean_squared_error(y, oof_predictions)\n# oof_rmsle = np.sqrt(mean_squared_log_error(y, oof_predictions))\n\n# print(f\"OOF Mean Squared Error: {oof_mse:.4f}\")\n# print(f\"OOF Root Mean Squared Log Error: {oof_rmsle:.4f}\")\n\n# # Output predictions\n# print(\"Final Test Predictions:\", final_test_predictions)\n# print(oof_results_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:18:13.574032Z","iopub.execute_input":"2024-12-30T12:18:13.574363Z","iopub.status.idle":"2024-12-30T12:18:13.592171Z","shell.execute_reply.started":"2024-12-30T12:18:13.574334Z","shell.execute_reply":"2024-12-30T12:18:13.591048Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# sub = pd.read_csv('/kaggle/input/playground-series-s4e12/sample_submission.csv')\n# sub['Premium Amount'] = final_test_predictions\n# sub.to_csv('submission.csv', index=False)\n# oof_results_df.to_csv('oof_aaba.csv',index = False)\n# sub.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-30T12:18:13.593289Z","iopub.execute_input":"2024-12-30T12:18:13.593607Z","iopub.status.idle":"2024-12-30T12:18:13.615737Z","shell.execute_reply.started":"2024-12-30T12:18:13.593581Z","shell.execute_reply":"2024-12-30T12:18:13.614453Z"}},"outputs":[],"execution_count":28},{"cell_type":"markdown","source":"# Ensembling","metadata":{}},{"cell_type":"code","source":"gydhsgkjdf","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}